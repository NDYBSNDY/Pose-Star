{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcafe28c-9f59-4d96-b52f-3e3f45a16c3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Copyright 2022 Google LLC. Double-click for license information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952f9ab-ed6f-4e99-8304-99a3716734b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b343b-1c90-4747-a753-71eb7071a289",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Null-text inversion + Editing with Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1b6199-9dfe-4055-8a84-66ff4bfa8901",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils\n",
    "import seq_aligner\n",
    "import shutil\n",
    "from torch.optim.adam import Adam\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a4bd1-3130-408b-ae2d-a166b9f19cb7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update MY_TOKEN with your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558a4b4-fec6-4bd2-9c8f-139809b1a1a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype']. These kwargs are not used in <class 'diffusers.quantizers.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c932381c647447b9a760be92d7683355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd5edd023ff46df8be980ca71e1afc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "MY_TOKEN = ''\n",
    "LOW_RESOURCE = False \n",
    "# NUM_DDIM_STEPS = 50\n",
    "NUM_DDIM_STEPS = 100\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "from diffusers import BitsAndBytesConfig, SD3Transformer2DModel\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "import torch\n",
    "model_id = \"/home/dongyuran/code/Our_mask/infer/stabilityai/stable-diffusion-3.5-large\"\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model_nf4 = SD3Transformer2DModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=nf4_config,\n",
    "    # torch_dtype=torch.bfloat16\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "ldm_stable = StableDiffusion3Pipeline.from_pretrained(\n",
    "    model_id, \n",
    "    transformer=model_nf4,\n",
    "    # torch_dtype=torch.bfloat16\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "ldm_stable.enable_model_cpu_offload()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01422991-cafe-4cf0-8406-66f052a75d9b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prompt-to-Prompt code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc083590-15de-4216-8d8d-50336f9f1d34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class LocalBlend:\n",
    "    \n",
    "    def get_mask(self, maps, alpha, use_pool):\n",
    "        k = 1\n",
    "        maps = (maps * alpha).sum(-1).mean(1)\n",
    "        if use_pool:\n",
    "            maps = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(maps, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.th[1-int(use_pool)])\n",
    "        mask = mask[:1] + mask\n",
    "        return mask\n",
    "    \n",
    "    def __call__(self, x_t, attention_store):\n",
    "        self.counter += 1\n",
    "        if self.counter > self.start_blend:\n",
    "           \n",
    "            maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "            # maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
    "            maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 64, 64, MAX_NUM_WORDS) for item in maps]\n",
    "            maps = torch.cat(maps, dim=1)\n",
    "            mask = self.get_mask(maps, self.alpha_layers, True)\n",
    "            if self.substruct_layers is not None:\n",
    "                maps_sub = ~self.get_mask(maps, self.substruct_layers, False)\n",
    "                mask = mask * maps_sub\n",
    "            mask = mask.float()\n",
    "            x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "       \n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], substruct_words=None, start_blend=0.2, th=(.3, .3)):\n",
    "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        \n",
    "        if substruct_words is not None:\n",
    "            substruct_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "            for i, (prompt, words_) in enumerate(zip(prompts, substruct_words)):\n",
    "                if type(words_) is str:\n",
    "                    words_ = [words_]\n",
    "                for word in words_:\n",
    "                    ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                    substruct_layers[i, :, :, :, :, ind] = 1\n",
    "            self.substruct_layers = substruct_layers.to(device)\n",
    "        else:\n",
    "            self.substruct_layers = None\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.start_blend = int(start_blend * NUM_DDIM_STEPS)\n",
    "        self.counter = 0 \n",
    "        self.th=th\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class EmptyControl:\n",
    "    \n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "\n",
    "    \n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class SpatialReplace(EmptyControl):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.cur_step < self.stop_inject:\n",
    "            b = x_t.shape[0]\n",
    "            x_t = x_t[:1].expand(b, *x_t.shape[1:])\n",
    "        return x_t\n",
    "\n",
    "    def __init__(self, stop_inject: float):\n",
    "        super(SpatialReplace, self).__init__()\n",
    "        self.stop_inject = int((1 - stop_inject) * NUM_DDIM_STEPS)\n",
    "        \n",
    "\n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        \n",
    "        # 获取每个扩散步骤的注意力图\n",
    "        res = 16\n",
    "        # res = 32\n",
    "        from_where = (\"up\", \"down\")\n",
    "        # aggregate_attention\n",
    "        out = []\n",
    "        is_cross = True\n",
    "        attention_maps = {key: [item / 1 for item in self.step_store[key]] for key in self.step_store}\n",
    "        num_pixels = res ** 2\n",
    "        for location in from_where:\n",
    "            for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "                if item.shape[1] == num_pixels:\n",
    "                    cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[0]\n",
    "                    out.append(cross_maps)\n",
    "        out = torch.cat(out, dim=0)\n",
    "        out = out.sum(0) / out.shape[0]\n",
    "        out = out.cpu()\n",
    "        # show_cross_attention\n",
    "        tokens = tokenizer.encode(prompts[0])\n",
    "        decoder = tokenizer.decode\n",
    "        attention_maps = out\n",
    "        images = []\n",
    "        for i in range(len(tokens)):\n",
    "            image = attention_maps[:, :, i]\n",
    "            image = 255 * image / image.max()\n",
    "            image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "            image = image.numpy().astype(np.uint8)\n",
    "            # if i == 1:\n",
    "            #     c = Image.fromarray(image).resize((512, 512))\n",
    "            #     c.save(\"/home/dongyuran/code/prompt-to-prompt-main/diffusion_step/\" + str(self.cur_step) + '.png')\n",
    "            c = Image.fromarray(image).resize((512, 512))\n",
    "            # c.save(\"/home/dongyuran/code/Our_mask/infer/res/diffusion_step_all/\" + str(i) + \"_\" + str(self.cur_step) + '.png')\n",
    "            c.save(\"/home/dongyuran/code/Our_mask/infer/res/diffusion_step_all-1/\" + str(i) + \"_\" + str(self.cur_step) + '.png')\n",
    "            image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "            image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "            images.append(image)\n",
    "        ptp_utils.view_images(np.stack(images, axis=0)) \n",
    "\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "        \n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "        \n",
    "    def replace_self_attention(self, attn_base, att_replace, place_in_unet):\n",
    "        if att_replace.shape[2] <= 32 ** 2:\n",
    "            attn_base = attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "            return attn_base\n",
    "        else:\n",
    "            return att_replace\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce, place_in_unet)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "    \n",
    "    def __init__(self, prompts, num_steps: int,\n",
    "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
    "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
    "                 local_blend: Optional[LocalBlend]):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
    "      \n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "        \n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
    "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
    "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
    "                  Tuple[float, ...]]):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(1, 77)\n",
    "    \n",
    "    for word, val in zip(word_select, values):\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = val\n",
    "    return equalizer\n",
    "\n",
    "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res ** 2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                a = len(prompts)\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
    "                out.append(cross_maps) \n",
    "    out = torch.cat(out, dim=0) \n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def make_controller(prompts: List[str], is_replace_controller: bool, cross_replace_steps: Dict[str, float], self_replace_steps: float, blend_words=None, equilizer_params=None) -> AttentionControlEdit:\n",
    "    if blend_words is None:\n",
    "        lb = None\n",
    "    else:\n",
    "        lb = LocalBlend(prompts, blend_word)\n",
    "    if is_replace_controller:\n",
    "        controller = AttentionReplace(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
    "    else:\n",
    "        controller = AttentionRefine(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
    "    if equilizer_params is not None:\n",
    "        eq = get_equalizer(prompts[1], equilizer_params[\"words\"], equilizer_params[\"values\"])\n",
    "        controller = AttentionReweight(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps,\n",
    "                                       self_replace_steps=self_replace_steps, equalizer=eq, local_blend=lb, controller=controller)\n",
    "    return controller\n",
    "\n",
    "\n",
    "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "    \n",
    "\n",
    "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
    "                        max_com=10, select: int = 0):\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
    "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914bda0-c191-4db6-b891-101cde74ddaf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Null Text Inversion code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c442992d-8156-4dfc-a2a5-1fbf8bedb4b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_512(image_path, left=0, right=0, top=0, bottom=0):\n",
    "    if type(image_path) is str:\n",
    "        image = np.array(Image.open(image_path))[:, :, :3]\n",
    "    else:\n",
    "        image = image_path\n",
    "    h, w, c = image.shape\n",
    "    left = min(left, w-1)\n",
    "    right = min(right, w - left - 1)\n",
    "    top = min(top, h - left - 1)\n",
    "    bottom = min(bottom, h - top - 1)\n",
    "    image = image[top:h-bottom, left:w-right]\n",
    "    h, w, c = image.shape\n",
    "    if h < w:\n",
    "        offset = (w - h) // 2\n",
    "        image = image[:, offset:offset + h]\n",
    "    elif w < h:\n",
    "        offset = (h - w) // 2\n",
    "        image = image[offset:offset + w]\n",
    "    image = np.array(Image.fromarray(image).resize((512, 512)))\n",
    "    return image\n",
    "\n",
    "\n",
    "class NullInversion:\n",
    "    \n",
    "    def prev_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        pred_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev) ** 0.5 * model_output\n",
    "        prev_sample = alpha_prod_t_prev ** 0.5 * pred_original_sample + pred_sample_direction\n",
    "        return prev_sample\n",
    "    \n",
    "    def next_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        timestep, next_timestep = min(timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps, 999), timestep\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep] if timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep]\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
    "        next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
    "        return next_sample\n",
    "    \n",
    "    # def get_noise_pred_single(self, latents, t, context):\n",
    "    #     noise_pred = self.model.unet(latents, t, encoder_hidden_states=context)[\"sample\"]\n",
    "    #     return noise_pred\n",
    "    def get_noise_pred_single(self, latents, t, context):\n",
    "        # context = torch.rand(1,77,768) \n",
    "        # noise_pred = self.model.transformer(latents, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        do_classifier_free_guidance = False\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        timestep = t.expand(latent_model_input.shape[0])\n",
    "        # noise_pred = self.model.transformer(hidden_states=latents, timestep=t, encoder_hidden_states=context)[\"sample\"]\n",
    "        joint_attention_kwargs = None\n",
    "        noise_pred = self.model.transformer(\n",
    "                    hidden_states=latent_model_input,\n",
    "                    timestep=timestep,\n",
    "                    encoder_hidden_states=self.prompt_embeds,\n",
    "                    pooled_projections=self.pooled_prompt_embeds,\n",
    "                    joint_attention_kwargs=joint_attention_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "        noise_pred = self.model.transformer(hidden_states=latent_model_input, timestep=timestep, encoder_hidden_states=context)[0]\n",
    "        return noise_pred\n",
    "\n",
    "    # def get_noise_pred(self, latents, t, is_forward=True, context=None):\n",
    "    #     latents_input = torch.cat([latents] * 2)\n",
    "    #     if context is None:\n",
    "    #         context = self.context\n",
    "    #     guidance_scale = 1 if is_forward else GUIDANCE_SCALE\n",
    "    #     noise_pred = self.model.unet(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
    "    #     noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "    #     noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
    "    #     if is_forward:\n",
    "    #         latents = self.next_step(noise_pred, t, latents)\n",
    "    #     else:\n",
    "    #         latents = self.prev_step(noise_pred, t, latents)\n",
    "    #     return latents\n",
    "    \n",
    "    def get_noise_pred(self, latents, t, is_forward=True, context=None):\n",
    "        latents_input = torch.cat([latents] * 2)\n",
    "        if context is None:\n",
    "            context = self.context\n",
    "        guidance_scale = 1 if is_forward else GUIDANCE_SCALE\n",
    "        noise_pred = self.model.transformer(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
    "        if is_forward:\n",
    "            latents = self.next_step(noise_pred, t, latents)\n",
    "        else:\n",
    "            latents = self.prev_step(noise_pred, t, latents)\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2image(self, latents, return_type='np'):\n",
    "        latents = 1 / 0.18215 * latents.detach()\n",
    "        image = self.model.vae.decode(latents)['sample']\n",
    "        if return_type == 'np':\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def image2latent(self, image):\n",
    "        with torch.no_grad():\n",
    "            if type(image) is Image:\n",
    "                image = np.array(image)\n",
    "            if type(image) is torch.Tensor and image.dim() == 4:\n",
    "                latents = image\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float() / 127.5 - 1\n",
    "                image = image.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                latents = self.model.vae.encode(image)['latent_dist'].mean\n",
    "                latents = latents * 0.18215\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uncond_input = self.model.tokenizer(\n",
    "            [\"\"], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings = self.model.text_encoder(uncond_input.input_ids.to(self.model.device))[0]\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
    "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.prompt = prompt\n",
    "\n",
    "        # ---------------------------------\n",
    "        num_images_per_prompt: int = 1\n",
    "        max_sequence_length: int = 512\n",
    "        batch_size = 1\n",
    "        text_inputs = self.model.tokenizer_3(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_sequence_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        prompt_embeds = self.model.text_encoder_3(text_input_ids.to(device))[0]\n",
    "        dtype = self.model.text_encoder_3.dtype\n",
    "        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n",
    "        _, seq_len, _ = prompt_embeds.shape\n",
    "        pooled_prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "        self.prompt_embeds = prompt_embeds\n",
    "        self.pooled_prompt_embeds = pooled_prompt_embeds.view(-1, 2048)\n",
    "        # ---------------------------------\n",
    "\n",
    "    @torch.no_grad()                                     \n",
    "    def ddim_loop(self, latent):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        all_latent = [latent]\n",
    "        latent = latent.clone().detach()\n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            t = self.model.scheduler.timesteps[len(self.model.scheduler.timesteps) - i - 1]\n",
    "            noise_pred = self.get_noise_pred_single(latent, t, cond_embeddings)\n",
    "            latent = self.next_step(noise_pred, t, latent)\n",
    "            all_latent.append(latent)\n",
    "        return all_latent\n",
    "\n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        return self.model.scheduler\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, image):\n",
    "        latent = self.image2latent(image)\n",
    "        image_rec = self.latent2image(latent)\n",
    "        ddim_latents = self.ddim_loop(latent)\n",
    "        return image_rec, ddim_latents\n",
    "\n",
    "    def null_optimization(self, latents, num_inner_steps, epsilon):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        uncond_embeddings_list = []\n",
    "        latent_cur = latents[-1]\n",
    "        bar = tqdm(total=num_inner_steps * NUM_DDIM_STEPS)\n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            uncond_embeddings = uncond_embeddings.clone().detach()\n",
    "            uncond_embeddings.requires_grad = True\n",
    "            # optimizer = Adam([uncond_embeddings], lr=1e-2 * (1. - i / 100.))\n",
    "            optimizer = Adam([uncond_embeddings], lr=1e-2 * (1. - i / 1000.))\n",
    "            latent_prev = latents[len(latents) - i - 2]\n",
    "            t = self.model.scheduler.timesteps[i]\n",
    "            with torch.no_grad():\n",
    "                noise_pred_cond = self.get_noise_pred_single(latent_cur, t, cond_embeddings)\n",
    "            for j in range(num_inner_steps):\n",
    "                noise_pred_uncond = self.get_noise_pred_single(latent_cur, t, uncond_embeddings)\n",
    "                noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_cond - noise_pred_uncond)\n",
    "                latents_prev_rec = self.prev_step(noise_pred, t, latent_cur)\n",
    "                loss = nnf.mse_loss(latents_prev_rec, latent_prev)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_item = loss.item()\n",
    "                bar.update()\n",
    "                if loss_item < epsilon + i * 2e-5:\n",
    "                    break\n",
    "            for j in range(j + 1, num_inner_steps):\n",
    "                bar.update()\n",
    "            uncond_embeddings_list.append(uncond_embeddings[:1].detach())\n",
    "            with torch.no_grad():\n",
    "                context = torch.cat([uncond_embeddings, cond_embeddings])\n",
    "                latent_cur = self.get_noise_pred(latent_cur, t, False, context)\n",
    "        bar.close()\n",
    "        return uncond_embeddings_list\n",
    "    \n",
    "    def invert(self, image_path: str, prompt: str, offsets=(0,0,0,0), num_inner_steps=10, early_stop_epsilon=1e-5, verbose=False):\n",
    "        self.init_prompt(prompt)\n",
    "        ptp_utils.register_attention_control(self.model, None)\n",
    "        image_gt = load_512(image_path, *offsets)\n",
    "        if verbose:\n",
    "            print(\"DDIM inversion...\")\n",
    "        image_rec, ddim_latents = self.ddim_inversion(image_gt)\n",
    "        if verbose:\n",
    "            print(\"Null-text optimization...\")\n",
    "        uncond_embeddings = self.null_optimization(ddim_latents, num_inner_steps, early_stop_epsilon)\n",
    "        return (image_gt, image_rec), ddim_latents[-1], uncond_embeddings\n",
    "        \n",
    "    \n",
    "    def __init__(self, model):\n",
    "        scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
    "                                  set_alpha_to_one=False)\n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.model.scheduler.set_timesteps(NUM_DDIM_STEPS)\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "\n",
    "null_inversion = NullInversion(ldm_stable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919e093-998c-4e4c-92a2-dc9517ef8ea4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Infernce Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9499145-1a2b-4c91-900e-093c0c08043c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def text2image_ldm_stable(\n",
    "    model,\n",
    "    prompt:  List[str],\n",
    "    controller,\n",
    "    # num_inference_steps: int = 50,\n",
    "    num_inference_steps: int = 100,\n",
    "    guidance_scale: Optional[float] = 7.5,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    "    latent: Optional[torch.FloatTensor] = None,\n",
    "    uncond_embeddings=None,\n",
    "    # start_time = 50,\n",
    "    start_time = 100,\n",
    "    return_type='image'\n",
    "):\n",
    "    batch_size = len(prompt)\n",
    "    ptp_utils.register_attention_control(model, controller)\n",
    "    height = width = 512\n",
    "    \n",
    "    text_input = model.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    if uncond_embeddings is None:\n",
    "        uncond_input = model.tokenizer(\n",
    "            [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings_ = model.text_encoder(uncond_input.input_ids.to(model.device))[0]\n",
    "    else:\n",
    "        uncond_embeddings_ = None\n",
    "\n",
    "    latent, latents = ptp_utils.init_latent(latent, model, height, width, generator, batch_size)\n",
    "    model.scheduler.set_timesteps(num_inference_steps)\n",
    "    for i, t in enumerate(tqdm(model.scheduler.timesteps[-start_time:])):\n",
    "        if uncond_embeddings_ is None:\n",
    "            context = torch.cat([uncond_embeddings[i].expand(*text_embeddings.shape), text_embeddings])\n",
    "        else:\n",
    "            context = torch.cat([uncond_embeddings_, text_embeddings])\n",
    "        latents = ptp_utils.diffusion_step(model, controller, latents, context, t, guidance_scale, low_resource=False)\n",
    "        \n",
    "    if return_type == 'image':\n",
    "        image = ptp_utils.latent2image(model.vae, latents)\n",
    "    else:\n",
    "        image = latents\n",
    "    return image, latent\n",
    "\n",
    "\n",
    "\n",
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None, uncond_embeddings=None, verbose=True):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DDIM_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, uncond_embeddings=uncond_embeddings)\n",
    "    if verbose:\n",
    "        ptp_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5cbc6-2581-415f-b608-67f2e87c32f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDIM inversion...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.50 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2857248/664276550.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Neck to hands\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# prompt = \"Upper and lower body\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mimage_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncond_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnull_inversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Modify or remove offsets according to your image!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2857248/1730908105.py\u001b[0m in \u001b[0;36minvert\u001b[0;34m(self, image_path, prompt, offsets, num_inner_steps, early_stop_epsilon, verbose)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DDIM inversion...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mimage_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddim_latents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddim_inversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Null-text optimization...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2857248/1730908105.py\u001b[0m in \u001b[0;36mddim_inversion\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage2latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mimage_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mddim_latents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddim_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddim_latents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2857248/1730908105.py\u001b[0m in \u001b[0;36mddim_loop\u001b[0;34m(self, latent)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_DDIM_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mnoise_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_noise_pred_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mall_latent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2857248/1730908105.py\u001b[0m in \u001b[0;36mget_noise_pred_single\u001b[0;34m(self, latents, t, context)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# noise_pred = self.model.transformer(hidden_states=latents, timestep=t, encoder_hidden_states=context)[\"sample\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mjoint_attention_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         noise_pred = self.model.transformer(\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/diffusers/models/transformers/transformer_sd3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, pooled_projections, timestep, block_controlnet_hidden_states, joint_attention_kwargs, return_dict, skip_layers)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 )\n\u001b[1;32m    425\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_skip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                 encoder_hidden_states, hidden_states = block(\n\u001b[0m\u001b[1;32m    427\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, temb, joint_attention_kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m             )\n\u001b[1;32m    202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_msa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_pre_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p2p_diffusers/lib/python3.8/site-packages/diffusers/models/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, timestep, class_labels, hidden_dtype, emb)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mshift_msa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_msa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_msa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscale_msa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshift_msa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_msa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_mlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.50 GiB. GPU "
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = \"./example_images/people-2.png\"\n",
    "\n",
    "prompt = \"Neck to hands\"\n",
    "\n",
    "(image_gt, image_enc), x_t, uncond_embeddings = null_inversion.invert(image_path, prompt, offsets=(0,0,200,0), verbose=True)\n",
    "\n",
    "print(\"Modify or remove offsets according to your image!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d450d-6764-4195-9ff1-842b2f60249e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [prompt]\n",
    "controller = AttentionStore()\n",
    "image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, verbose=False)\n",
    "print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, the null-text inverted image\")\n",
    "ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
    "show_cross_attention(controller, 16, [\"up\", \"down\"])\n",
    "# show_cross_attention(controller, 32, [\"up\", \"down\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da742d87-b48e-40e1-8b8e-c0f9b7528cc9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"a cat sitting next to a mirror\",\n",
    "           \"a tiger sitting next to a mirror\"\n",
    "        ]\n",
    "\n",
    "cross_replace_steps = {'default_': .8,}\n",
    "self_replace_steps = .5\n",
    "blend_word = ((('cat',), (\"tiger\",))) # for local edit. If it is not local yet - use only \n",
    "eq_params = {\"words\": (\"tiger\",), \"values\": (2,)} # amplify attention to the word \"tiger\" by *2 \n",
    "\n",
    "controller = make_controller(prompts, True, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)\n",
    "\n",
    "print(\"Image is highly affected by the self_replace_steps, usually 0.4 is a good default value, but you may want to try the range 0.3,0.4,0.5,0.7 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861db4be-72cc-4d8f-969b-bba1bf45bb50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"a cat sitting next to a mirror\",\n",
    "           \"a silver cat sculpture sitting next to a mirror\"\n",
    "        ]\n",
    "\n",
    "cross_replace_steps = {'default_': .8, }\n",
    "self_replace_steps = .6\n",
    "blend_word = ((('cat',), (\"cat\",))) # for local edit\n",
    "eq_params = {\"words\": (\"silver\", 'sculpture', ), \"values\": (2,2,)}  # amplify attention to the words \"silver\" and \"sculpture\" by *2 \n",
    " \n",
    "controller = make_controller(prompts, False, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b768dd2-a139-4163-823e-15318441ea49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"a cat sitting next to a mirror\",\n",
    "           \"watercolor painting of a cat sitting next to a mirror\"\n",
    "        ]\n",
    "\n",
    "cross_replace_steps = {'default_': .8, }\n",
    "self_replace_steps = .7\n",
    "blend_word = None\n",
    "eq_params = {\"words\": (\"watercolor\",  ), \"values\": (5, 2,)}  # amplify attention to the word \"watercolor\" by 5\n",
    " \n",
    "controller = make_controller(prompts, False, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfb554-148b-45af-ae8b-17b213f9070f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
